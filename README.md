# 小说爬虫工具

一个使用Selenium自动化爬取小说网站内容的Python脚本，可以从指定章节开始，自动爬取多章小说内容并保存到本地文件。

## 功能特点

- 🤖 **自动化爬取**：使用Selenium自动化浏览器操作，无需手动干预
- 🔧 **多选择器支持**：内置多种内容和下一章链接的定位方式，提高兼容性
- 📄 **智能保存**：自动将爬取的小说内容保存到本地文本文件
- ⏭️ **章节跳过**：支持从指定章节开始爬取，无需从头开始
- 📊 **进度显示**：实时显示爬取进度和状态信息
- 🔍 **错误处理**：完善的异常捕获和错误处理机制

## 安装说明

### 1. 安装Python

确保你的电脑已经安装了Python 3.7或更高版本。

### 2. 安装依赖

使用pip安装所需的Python库：

```bash
pip install selenium
```

### 3. 下载Chrome浏览器

确保你的电脑已经安装了Chrome浏览器。

### 4. 下载Chrome驱动

下载与你Chrome浏览器版本匹配的ChromeDriver：
- 查看Chrome版本：在Chrome中输入 `chrome://version/`
- 下载地址：[ChromeDriver官网](https://sites.google.com/chromium.org/driver/)

## 使用方法

### 1. 配置脚本参数

打开 `1.py` 文件，修改以下参数：

- `start_url`：小说网站的起始URL
- `start_chapter`：开始爬取的章节号
- `target_chapters`：目标爬取的章节数量

### 2. 运行脚本

在命令行中执行以下命令：

```bash
python 1.py
```

### 3. 查看结果

脚本执行完成后，会在当前目录生成 `小说完整内容.txt` 文件，包含爬取的所有小说章节内容。

## 配置参数说明

| 参数名 | 说明 | 默认值 |
|--------|------|--------|
| `start_url` | 小说网站的起始URL | `https://www.02a418d2.cfd/#/book/1901/315.html` |
| `start_chapter` | 开始爬取的章节号 | `30` |
| `target_chapters` | 目标爬取的章节数量 | `40` |
| `chrome_options` | Chrome浏览器配置 | 无头模式、禁用GPU等 |

## 工作原理

1. **初始化浏览器**：使用Selenium启动Chrome浏览器（无头模式）
2. **打开起始页面**：访问指定的小说URL
3. **跳过章节**：自动跳转到指定的开始章节
4. **爬取内容**：使用多种选择器尝试查找小说内容
5. **保存内容**：将爬取到的内容写入本地文件
6. **查找下一章**：使用多种选择器尝试查找下一章链接
7. **循环爬取**：重复步骤4-6，直到达到目标章节数量
8. **结束爬取**：关闭浏览器，输出爬取结果统计

## 注意事项

1. **网站兼容性**：该脚本针对特定网站开发，可能需要根据不同网站调整选择器
2. **爬取速度**：为避免对网站造成过大压力，脚本每次请求后会等待5秒
3. **Chrome版本**：确保Chrome浏览器与ChromeDriver版本匹配
4. **网络环境**：请确保你的网络环境可以正常访问目标网站
5. **法律合规**：请遵守网站的 robots.txt 协议，合法使用本脚本
6. **内容版权**：爬取的小说内容仅供个人学习使用，请勿用于商业用途

## 常见问题

### Q: 运行脚本时提示找不到ChromeDriver？
A: 请确保ChromeDriver已正确安装，并添加到系统环境变量中。

### Q: 脚本运行时提示"未找到小说内容"？
A: 可能是目标网站结构发生变化，请根据网站实际情况调整内容选择器。

### Q: 脚本运行时提示"未找到下一章链接"？
A: 可能是目标网站的下一章链接选择器发生变化，请根据网站实际情况调整下一章选择器。

## 许可证

本项目采用 MIT 许可证，详情请查看 [LICENSE](LICENSE) 文件。

## 贡献

欢迎提交 Issue 和 Pull Request，一起完善这个项目！

## 更新日志

### v1.0.0 (2025-12-05)
- 初始版本发布
- 支持从指定章节开始爬取
- 支持多章节自动爬取
- 支持多种内容和下一章链接定位方式
- 完善的错误处理机制
- 实时进度显示

## 联系方式

如有问题或建议，欢迎通过以下方式联系：

- 提交 Issue
- 发送邮件:1563042919@qq.com

---

**使用本脚本即表示你同意遵守相关法律法规，合法使用爬虫工具。**
